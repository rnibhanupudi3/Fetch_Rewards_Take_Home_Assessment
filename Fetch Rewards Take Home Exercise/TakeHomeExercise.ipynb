{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73ba791",
   "metadata": {},
   "source": [
    "# Fetch Rewards: ML Apprentice Take-Home Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44dd736",
   "metadata": {},
   "source": [
    "### Please see write-up for rationale + technical justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8032a236-c689-4df6-8026-29b8a13ecac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Package installs -> Uncomment when running the first time!\n",
    "\n",
    "# ! pip install --quiet torch\n",
    "# ! pip install --quiet numpy\n",
    "# ! pip install --quiet transformers\n",
    "# # ! pip install --quiet hf_xet\n",
    "# ! pip install --quiet scikit-learn\n",
    "# # ! pip install --user --quiet ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc06b40-20b9-4c42-895f-7c2dafc1902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nibha\\anaconda3\\envs\\new_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88199f-d359-46b6-9af9-d7a86de08e7e",
   "metadata": {},
   "source": [
    "## Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55838515-c471-4fde-a4cc-a74ae4279428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\"\n",
    "    Compute mean pooling of token embeddings, ignoring padding tokens.\n",
    "    \n",
    "    Args:\n",
    "        token_embeddings: Tensor of shape [batch_size, seq_len, hidden_size]\n",
    "        attention_mask: Tensor of shape [batch_size, seq_len] with 1s for tokens and 0s for padding\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape [batch_size, hidden_size] with sentence embeddings\n",
    "    \"\"\"\n",
    "    # Convert attention mask to float for multiplication\n",
    "    mask = attention_mask.unsqueeze(-1).float()\n",
    "    \n",
    "    # Sum the embeddings of real tokens (multiply by mask to zero out padding)\n",
    "    sum_embeddings = torch.sum(token_embeddings * mask, dim=1)\n",
    "    \n",
    "    # Count the number of real tokens per sentence\n",
    "    token_counts = torch.sum(attention_mask, dim=1, keepdim=True).float()\n",
    "    \n",
    "    # Compute mean by dividing sum by count\n",
    "    sentence_embeddings = sum_embeddings / token_counts\n",
    "    \n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def max_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\"\n",
    "    Compute max pooling of token embeddings, ignoring padding tokens.\n",
    "    \n",
    "    Args:\n",
    "        token_embeddings: Tensor of shape [batch_size, seq_len, hidden_size]\n",
    "        attention_mask: Tensor of shape [batch_size, seq_len] with 1s for tokens and 0s for padding\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape [batch_size, hidden_size] with sentence embeddings\n",
    "    \"\"\"\n",
    "    # Create a mask for padding tokens (0s for padding, 1s for real tokens)\n",
    "    mask = attention_mask.unsqueeze(-1).expand_as(token_embeddings).float()\n",
    "    \n",
    "    # Replace padding token embeddings with large negative values\n",
    "    # This ensures they won't be selected during max operation\n",
    "    masked_embeddings = token_embeddings.clone()\n",
    "    masked_embeddings[mask == 0] = -1e9\n",
    "    \n",
    "    # Take max over sequence dimension (dim=1)\n",
    "    sentence_embeddings = torch.max(masked_embeddings, dim=1)[0]\n",
    "    \n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8c1a93-fba2-4a97-b0e9-7f91225956c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768, pooling_strategy='mean', normalize_embeddings=True):\n",
    "        \"\"\"\n",
    "        Initialize the Sentence Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Pre-trained transformer model name\n",
    "            embedding_dim: Dimension of embeddings from the transformer model\n",
    "            pooling_strategy: Strategy to pool token embeddings into sentence embedding\n",
    "            projection_dim: If provided, project embeddings to this dimension\n",
    "            normalize_embeddings: Whether to normalize final embeddings\n",
    "        \"\"\"\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        \n",
    "        # Load pre-trained transformer model\n",
    "        self.transformer = BertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Configure pooling strategy\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "            \n",
    "        # Whether to normalize final embeddings\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        # Get transformer outputs\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get embeddings from the transformer output\n",
    "        token_embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Apply pooling strategy\n",
    "        if self.pooling_strategy == 'cls':\n",
    "            # Use [CLS] token embedding\n",
    "            sentence_embedding = token_embeddings[:, 0, :]\n",
    "        elif self.pooling_strategy == 'mean':\n",
    "            # Mean pooling - take mean of all BERT embeddings for a sentence\n",
    "            sentence_embedding = mean_pooling(token_embeddings, attention_mask)\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            # Max pooling - take max of all BERT embeddings for a sentence\n",
    "            sentence_embedding = max_pooling(token_embeddings, attention_mask)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling strategy: {self.pooling_strategy}\")\n",
    "        \n",
    "        # Normalize embeddings so that similar sentences can be compared via cosine similarity\n",
    "        if self.normalize_embeddings:\n",
    "            sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)\n",
    "            \n",
    "        return sentence_embedding\n",
    "\n",
    "    \n",
    "    def encode_single(self, sentence, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Encode a single sentence into an embedding\n",
    "        \n",
    "        Args:\n",
    "            sentence: String sentence to encode\n",
    "            device: Device to use for computation\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of sentence embedding\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        \n",
    "        # Tokenize the sentence\n",
    "        inputs = self.tokenizer(sentence, padding=True, truncation=True, \n",
    "                               return_tensors=\"pt\", max_length=512)\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = self.forward(inputs['input_ids'], inputs['attention_mask'])\n",
    "        \n",
    "        # Move embedding to CPU and convert to numpy\n",
    "        return embedding.detach().cpu().numpy()[0]  # Get the first (and only) item in batch\n",
    "\n",
    "\n",
    "    def encode(self, sentences, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Encode a list of sentences into embeddings, processing one sentence at a time\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences to encode\n",
    "            device: Device to use for computation\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of sentence embeddings\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process sentences one at a time\n",
    "        for sentence in sentences:\n",
    "            embedding = self.encode_single(sentence, device)\n",
    "            all_embeddings.append(embedding)\n",
    "        \n",
    "        # Stack all embeddings\n",
    "        all_embeddings = np.vstack(all_embeddings)\n",
    "        \n",
    "        return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd981ff6-7fe9-4bdd-986c-de9ec0865f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: BERT-base with mean pooling\n",
      "Embedding shape: (3, 768), Number of sentences: 3\n",
      "\n",
      "Sample embeddings (first 5 dimensions):\n",
      "Sentence: The new smartphone features an AI-powered camera and improved battery life.\n",
      "Embedding: [-0.01046181 -0.00928617  0.05656587 -0.00583826  0.03553018]...\n",
      "\n",
      "Sentence: The team scored a last-minute goal to win the championship.\n",
      "Embedding: [-0.0263459  -0.05063563  0.02050924 -0.02016847  0.02163503]...\n",
      "\n",
      "Sentence: The senator proposed a new bill to address climate change.\n",
      "Embedding: [-0.00652519 -0.04245586 -0.03979944  0.00203729 -0.00385006]...\n",
      "\n",
      "\n",
      "Testing: BERT-base with CLS pooling\n",
      "Embedding shape: (3, 768), Number of sentences: 3\n",
      "\n",
      "Sample embeddings (first 5 dimensions):\n",
      "Sentence: The new smartphone features an AI-powered camera and improved battery life.\n",
      "Embedding: [-0.02002563 -0.02972431  0.01177708 -0.01269144 -0.00239184]...\n",
      "\n",
      "Sentence: The team scored a last-minute goal to win the championship.\n",
      "Embedding: [-0.03604771 -0.02874261 -0.0050998  -0.02510636 -0.01940357]...\n",
      "\n",
      "Sentence: The senator proposed a new bill to address climate change.\n",
      "Embedding: [-0.02549031 -0.02025629 -0.04751641  0.00798129 -0.02230588]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Generate the fixed-length sentence embeddings\"\"\"\n",
    "    # Initialize model with different configurations\n",
    "    models = {\n",
    "        \"BERT-base with mean pooling\": SentenceTransformer(\n",
    "            model_name='bert-base-uncased', \n",
    "            pooling_strategy='mean',\n",
    "            normalize_embeddings=True\n",
    "        ),\n",
    "        \"BERT-base with CLS pooling\": SentenceTransformer(\n",
    "            model_name='bert-base-uncased', \n",
    "            pooling_strategy='cls',\n",
    "            normalize_embeddings=True\n",
    "        ),\n",
    "    }\n",
    "        \n",
    "    # Sample sentences\n",
    "    sample_sentences = [\n",
    "        \"The new smartphone features an AI-powered camera and improved battery life.\",\n",
    "        \"The team scored a last-minute goal to win the championship.\",\n",
    "        \"The senator proposed a new bill to address climate change.\",\n",
    "        \"The movie received excellent reviews from critics and audiences alike.\",\n",
    "        \"Regular exercise and a balanced diet are essential for maintaining good health.\",\n",
    "        \"The software update includes security patches and performance improvements.\",\n",
    "        \"The athlete broke the world record in the 100-meter sprint.\",\n",
    "        \"The president will deliver a speech on economic policy tomorrow.\",\n",
    "        \"The concert tickets sold out within minutes of going on sale.\",\n",
    "        \"The study found a strong correlation between sleep quality and cognitive function.\"\n",
    "    ][:3]\n",
    "    \n",
    "    # Test each model configuration\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTesting: {model_name}\")\n",
    "        \n",
    "        # Encode sentences\n",
    "        embeddings = model.encode(sample_sentences)\n",
    "        \n",
    "        # Print embedding dimensions\n",
    "        print(f\"Embedding shape: {embeddings.shape}, Number of sentences: {len(sample_sentences)}\")\n",
    "        \n",
    "        print(\"\\nSample embeddings (first 5 dimensions):\")\n",
    "        for i, sentence in enumerate(sample_sentences):\n",
    "            print(f\"Sentence: {sentence}\")\n",
    "            print(f\"Embedding: {embeddings[i][:5]}...\")\n",
    "            print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd657719-0651-49ca-9601-d6a4ca44aeda",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb44bf-6af3-489e-b968-92ff14e896d9",
   "metadata": {},
   "source": [
    "### Task A: Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d37c4e0-eae1-4b04-8c57-297ac1c718eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, sentence_transformer, num_classes, hidden_layers=None, freeze_transformer=True):\n",
    "        \"\"\"\n",
    "        Initialize a classifier that uses sentence embeddings.\n",
    "        \n",
    "        Args:\n",
    "            sentence_transformer: SentenceTransformer model for creating embeddings\n",
    "            num_classes: Number of output classes for classification/sentiment analysis\n",
    "            hidden_layers: List of hidden layer sizes (if None, only a single classification layer is used)\n",
    "            freeze_transformer: Whether to freeze the transformer part during training\n",
    "        \"\"\"\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        \n",
    "        self.sentence_transformer = sentence_transformer\n",
    "        \n",
    "        # Freeze transformer parameters if specified\n",
    "        if freeze_transformer:\n",
    "            for param in self.sentence_transformer.transformer.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Get embedding dimension\n",
    "        embedding_dim = self.sentence_transformer.embedding_dim\n",
    "        \n",
    "        # Create classifier layers\n",
    "        if hidden_layers is None:\n",
    "            # Simple linear classifier\n",
    "            self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "        else:\n",
    "            # Multi-layer classifier\n",
    "            layers = []\n",
    "            input_dim = embedding_dim\n",
    "            \n",
    "            # Add hidden layers\n",
    "            for hidden_dim in hidden_layers:\n",
    "                layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(0.1))\n",
    "                input_dim = hidden_dim\n",
    "                \n",
    "            # Add output layer\n",
    "            layers.append(nn.Linear(input_dim, num_classes))\n",
    "            \n",
    "            self.classifier = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        # Get sentence embeddings\n",
    "        embeddings = self.sentence_transformer(input_ids, attention_mask)\n",
    "        \n",
    "        # Apply classifier\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def classify(self, sentences, class_names=None, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Classify a list of sentences\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences to classify\n",
    "            class_names: List of class names (if None, returns class indices)\n",
    "            device: Device to use for computation\n",
    "            \n",
    "        Returns:\n",
    "            List of predicted classes\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        # Process sentences one at a time\n",
    "        for sentence in sentences:\n",
    "            # Tokenize the sentence\n",
    "            inputs = self.sentence_transformer.tokenizer(sentence, padding=True, truncation=True, \n",
    "                                                        return_tensors=\"pt\", max_length=512)\n",
    "            \n",
    "            # Move inputs to device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                logits = self.forward(inputs['input_ids'], inputs['attention_mask'])\n",
    "                prediction = torch.argmax(logits, dim=1).item()\n",
    "            \n",
    "            # Convert to class name if provided\n",
    "            if class_names is not None:\n",
    "                prediction = class_names[prediction]\n",
    "                \n",
    "            all_predictions.append(prediction)\n",
    "        \n",
    "        return all_predictions\n",
    "    \n",
    "    def pred_probs(self, sentences, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Get class probabilities for a list of sentences\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences to classify\n",
    "            device: Device to use for computation\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array of class probabilities\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        \n",
    "        all_probs = []\n",
    "        \n",
    "        # Process sentences one at a time\n",
    "        for sentence in sentences:\n",
    "            # Tokenize the sentence\n",
    "            inputs = self.sentence_transformer.tokenizer(sentence, padding=True, truncation=True, \n",
    "                                                        return_tensors=\"pt\", max_length=512)\n",
    "            \n",
    "            # Move inputs to device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                logits = self.forward(inputs['input_ids'], inputs['attention_mask'])\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            all_probs.append(probs.cpu().numpy()[0])\n",
    "        \n",
    "        return np.array(all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669357ca-dc4d-4f66-aa61-20254eca53bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting classes...\n",
      "\n",
      "Predictions:\n",
      "Sentence: The new smartphone features an AI-powered camera and improved battery life.\n",
      "True class: Technology\n",
      "Predicted class: Health\n",
      "\n",
      "Sentence: The team scored a last-minute goal to win the championship.\n",
      "True class: Sports\n",
      "Predicted class: Health\n",
      "\n",
      "\n",
      "Class probabilities:\n",
      "Sentence: The new smartphone features an AI-powered camera and improved battery life.\n",
      "  Technology: 0.2088\n",
      "  Sports: 0.1837\n",
      "  Politics: 0.1790\n",
      "  Entertainment: 0.2088\n",
      "  Health: 0.2198\n",
      "\n",
      "Sentence: The team scored a last-minute goal to win the championship.\n",
      "  Technology: 0.2084\n",
      "  Sports: 0.1836\n",
      "  Politics: 0.1791\n",
      "  Entertainment: 0.2088\n",
      "  Health: 0.2201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sentence_classification():\n",
    "    \"\"\"Run the inference for the classification model with sentence embedding backbone\"\"\"\n",
    "    # Define a sample classification task\n",
    "    class_names = [\"Technology\", \"Sports\", \"Politics\", \"Entertainment\", \"Health\"]\n",
    "    \n",
    "    # Sample data\n",
    "    sample_sentences = [\n",
    "        \"The new smartphone features an AI-powered camera and improved battery life.\",\n",
    "        \"The team scored a last-minute goal to win the championship.\",\n",
    "        \"The senator proposed a new bill to address climate change.\",\n",
    "        \"The movie received excellent reviews from critics and audiences alike.\",\n",
    "        \"Regular exercise and a balanced diet are essential for maintaining good health.\",\n",
    "        \"The software update includes security patches and performance improvements.\",\n",
    "        \"The athlete broke the world record in the 100-meter sprint.\",\n",
    "        \"The president will deliver a speech on economic policy tomorrow.\",\n",
    "        \"The concert tickets sold out within minutes of going on sale.\",\n",
    "        \"The study found a strong correlation between sleep quality and cognitive function.\"\n",
    "    ]\n",
    "    \n",
    "    # Sample labels (indices of class_names)\n",
    "    sample_labels = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "    \n",
    "    # Initialize the sentence transformer\n",
    "    sentence_transformer = SentenceTransformer(\n",
    "        model_name='bert-base-uncased',\n",
    "        pooling_strategy='cls',\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    classifier = SentenceClassifier(\n",
    "        sentence_transformer=sentence_transformer,\n",
    "        num_classes=len(class_names),\n",
    "        hidden_layers=[256, 128],  # Two hidden layers\n",
    "        freeze_transformer=True    # Freeze transformer weights\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\nPredicting classes...\")\n",
    "    predictions = classifier.classify(sample_sentences[:2], class_names=class_names)\n",
    "    \n",
    "    # Print predictions\n",
    "    print(\"\\nPredictions:\")\n",
    "    for i, (sentence, true_class, pred_class) in enumerate(zip(sample_sentences[:2], \n",
    "                                                            [class_names[i] for i in sample_labels[:2]], \n",
    "                                                            predictions)):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"True class: {true_class}\")\n",
    "        print(f\"Predicted class: {pred_class}\")\n",
    "        print()\n",
    "    \n",
    "    # Get classification probabilities\n",
    "    print(\"\\nClass probabilities:\")\n",
    "    probs = classifier.pred_probs(sample_sentences[:2])  # Just show first two for brevity\n",
    "    for i, (sentence, prob) in enumerate(zip(sample_sentences[:2], probs)):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        for j, class_name in enumerate(class_names):\n",
    "            print(f\"  {class_name}: {prob[j]:.4f}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentence_classification()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d42d21-b0cb-4a5a-ad24-12753be40422",
   "metadata": {},
   "source": [
    "### Task B: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a36da9-8766-4f02-b2b9-4c9ddf68b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting classes...\n",
      "\n",
      "Predictions:\n",
      "Sentence: This product is absolutely terrible and completely failed to meet my expectations.\n",
      "True class: Negative\n",
      "Predicted class: Neutral\n",
      "\n",
      "Sentence: The service was okay, but nothing special to write home about.\n",
      "True class: Neutral\n",
      "Predicted class: Neutral\n",
      "\n",
      "\n",
      "Class probabilities:\n",
      "Sentence: This product is absolutely terrible and completely failed to meet my expectations.\n",
      "  Negative: 0.3247\n",
      "  Neutral: 0.3494\n",
      "  Positive: 0.3259\n",
      "\n",
      "Sentence: The service was okay, but nothing special to write home about.\n",
      "  Negative: 0.3245\n",
      "  Neutral: 0.3491\n",
      "  Positive: 0.3264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sentiment_analysis():\n",
    "    \"\"\"Run the inference for the sentiment analysis model with sentence embedding backbone\"\"\"\n",
    "    # Define sentiment analysis classes\n",
    "    class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    \n",
    "    # Sample data with varied sentiment\n",
    "    sample_sentences = [\n",
    "        \"This product is absolutely terrible and completely failed to meet my expectations.\",\n",
    "        \"The service was okay, but nothing special to write home about.\",\n",
    "        \"I'm absolutely thrilled with my purchase! Best decision I've made all year.\",\n",
    "        \"The customer support experience was frustrating and completely unhelpful.\",\n",
    "        \"The movie was fairly standard, pretty much what you'd expect from this genre.\",\n",
    "        \"The restaurant exceeded all my expectations - incredible food and impeccable service!\",\n",
    "        \"I regret spending money on this disappointing product.\",\n",
    "        \"The conference covered the topics that were advertised but lacked depth.\",\n",
    "        \"The hotel staff went above and beyond to make our stay memorable and special.\",\n",
    "        \"This app constantly crashes and has wasted hours of my time.\",\n",
    "        \"The book contains useful information presented in a straightforward manner.\",\n",
    "        \"The concert was an amazing experience that I'll remember for years to come.\"\n",
    "    ]\n",
    "    \n",
    "    # Sample labels (0=Negative, 1=Neutral, 2=Positive)\n",
    "    sample_labels = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "    \n",
    "    # Initialize the sentence transformer\n",
    "    sentence_transformer = SentenceTransformer(\n",
    "        model_name='bert-base-uncased',\n",
    "        pooling_strategy='cls',\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    classifier = SentenceClassifier(\n",
    "        sentence_transformer=sentence_transformer,\n",
    "        num_classes=len(class_names),\n",
    "        hidden_layers=[256, 128],  # Two hidden layers\n",
    "        freeze_transformer=True    # Freeze transformer weights\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\nPredicting classes...\")\n",
    "    predictions = classifier.classify(sample_sentences[:2], class_names=class_names)\n",
    "    \n",
    "    # Print predictions\n",
    "    print(\"\\nPredictions:\")\n",
    "    for i, (sentence, true_class, pred_class) in enumerate(zip(sample_sentences[:2], \n",
    "                                                            [class_names[i] for i in sample_labels[:2]], \n",
    "                                                            predictions)):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"True class: {true_class}\")\n",
    "        print(f\"Predicted class: {pred_class}\")\n",
    "        print()\n",
    "    \n",
    "    # Get classification probabilities\n",
    "    print(\"\\nClass probabilities:\")\n",
    "    probs = classifier.pred_probs(sample_sentences[:2])  # Just show first two for brevity\n",
    "    for i, (sentence, prob) in enumerate(zip(sample_sentences[:2], probs)):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        for j, class_name in enumerate(class_names):\n",
    "            print(f\"  {class_name}: {prob[j]:.4f}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentiment_analysis()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a548b-afbd-476c-a514-5239d988dcca",
   "metadata": {},
   "source": [
    "### Multi-Task Learning Expansion (Both Classification and Sentiment Analysis in one model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e66415-3202-4428-9557-79a3c745162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample classification task\n",
    "cls_classes = [\"Technology\", \"Sports\", \"Politics\", \"Entertainment\", \"Health\"]\n",
    "sentiment_classes = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "mt_sentences = [\n",
    "    # Technology - Negative\n",
    "    \"The new smartphone suffers from overheating issues and frequent app crashes.\",\n",
    "    # Sports - Neutral\n",
    "    \"The team played their final match of the season, ending in a 1-1 draw.\",\n",
    "    # Politics - Positive\n",
    "    \"The senator was praised for introducing an innovative bill to tackle climate change.\",\n",
    "    # Entertainment - Negative\n",
    "    \"The movie failed to impress critics, receiving mostly poor reviews.\",\n",
    "    # Health - Neutral\n",
    "    \"Regular exercise and a balanced diet are commonly recommended by health professionals.\",\n",
    "    # Technology - Positive\n",
    "    \"The software update significantly boosted performance and added exciting new features.\",\n",
    "    # Sports - Negative\n",
    "    \"The athlete was disqualified after a false start in the 100-meter sprint.\",\n",
    "    # Politics - Neutral\n",
    "    \"The president is scheduled to speak about economic policy at tomorrow’s event.\",\n",
    "    # Entertainment - Positive\n",
    "    \"The concert was a spectacular event, thrilling fans and selling out in minutes.\",\n",
    "    # Health - Negative\n",
    "    \"The study highlighted how poor sleep habits can impair brain function over time.\"\n",
    "]\n",
    "\n",
    "# Sample labels (indices of class_names)\n",
    "cls_labels = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "sentiment_labels = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4c8137f-5296-4f31-9834-cd767b1020d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    def __init__(self, sentence_transformer, num_classes, num_sentiment_classes=3, hidden_layers=None, freeze_transformer=True):\n",
    "        \"\"\"\n",
    "        Initialize a classifier that uses sentence embeddings.\n",
    "        \n",
    "        Args:\n",
    "            sentence_transformer: SentenceTransformer model for creating embeddings\n",
    "            num_classes: Number of output classes for classification\n",
    "            num_sentiment_classes: Number of output classes for sentiment analysis\n",
    "            hidden_layers: List of hidden layer sizes (if None, only a single classification layer is used)\n",
    "            freeze_transformer: Whether to freeze the transformer part during training\n",
    "        \"\"\"\n",
    "        super(MultiTaskSentenceTransformer, self).__init__()\n",
    "        self.sentence_transformer = sentence_transformer\n",
    "        if freeze_transformer:\n",
    "            for param in self.sentence_transformer.transformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        embedding_dim = self.sentence_transformer.embedding_dim\n",
    "\n",
    "        # Classification head\n",
    "        if hidden_layers is None:\n",
    "            self.classification_head = nn.Linear(embedding_dim, num_classes)\n",
    "            self.sentiment_head = nn.Linear(embedding_dim, num_sentiment_classes)\n",
    "        else:\n",
    "            layers = []\n",
    "            input_dim = embedding_dim\n",
    "            for hidden_dim in hidden_layers:\n",
    "                layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(0.1))\n",
    "                input_dim = hidden_dim\n",
    "            cls_head = layers + [nn.Linear(input_dim, num_classes)]\n",
    "            sent_head = layers + [nn.Linear(input_dim, num_sentiment_classes)]\n",
    "            self.classification_head = nn.Sequential(*cls_head)\n",
    "            self.sentiment_head = nn.Sequential(*sent_head)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Multitask model forward pass\"\"\"\n",
    "        embeddings = self.sentence_transformer(input_ids, attention_mask)\n",
    "        class_logits = self.classification_head(embeddings)\n",
    "        sentiment_logits = self.sentiment_head(embeddings)\n",
    "        return class_logits, sentiment_logits\n",
    "\n",
    "# Unified inference function\n",
    "def multitask_inference(model, sentences, class_names=None, sentiment_names=None, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "        Run unified inference to ouptut both the classification and sentiment analysis predictions.\n",
    "        \n",
    "        Args:\n",
    "            model: The Multi-Task transformer model\n",
    "            sentences: The sentences to run inference over\n",
    "            class_names: The class names for classification (in order of the class indices)\n",
    "            sentiment_names: The class names for sentiment analysis (in order of the sentiment class indices)\n",
    "            device: torch device to run model inference on (GPU if cuda is available, else CPU)\n",
    "        \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        inputs = model.sentence_transformer.tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            class_logits, sentiment_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "            # Get predicted class\n",
    "            class_pred_idx = torch.argmax(class_logits, dim=1).item()\n",
    "            sentiment_pred_idx = torch.argmax(sentiment_logits, dim=1).item()\n",
    "\n",
    "            # Map to class names if provided\n",
    "            class_pred = class_names[class_pred_idx] if class_names is not None else class_pred_idx\n",
    "            sentiment_pred = sentiment_names[sentiment_pred_idx] if sentiment_names is not None else sentiment_pred_idx\n",
    "\n",
    "        predictions.append({\n",
    "            'sentence': sentence,\n",
    "            'classification': class_pred,\n",
    "            'sentiment': sentiment_pred,\n",
    "            'classification_probs': F.softmax(class_logits, dim=1).cpu().numpy()[0],\n",
    "            'sentiment_probs': F.softmax(sentiment_logits, dim=1).cpu().numpy()[0]\n",
    "        })\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73df3984-ed8c-499c-aa59-f492706d295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The new smartphone suffers from overheating issues and frequent app crashes.\n",
      "Predicted class: Health with probs [0.20356297 0.18196648 0.20348822 0.20161143 0.2093709 ]\n",
      "Predicted sentiment: Negative with probs [0.3638863  0.29467952 0.34143424]\n",
      "\n",
      "Sentence: The team played their final match of the season, ending in a 1-1 draw.\n",
      "Predicted class: Health with probs [0.20371853 0.18196344 0.20314692 0.2020812  0.20908993]\n",
      "Predicted sentiment: Negative with probs [0.3651609 0.2940458 0.3407933]\n",
      "\n",
      "Sentence: The senator was praised for introducing an innovative bill to tackle climate change.\n",
      "Predicted class: Health with probs [0.20351812 0.18172985 0.20337825 0.20194069 0.20943314]\n",
      "Predicted sentiment: Negative with probs [0.3644743  0.29452685 0.34099886]\n",
      "\n",
      "Sentence: The movie failed to impress critics, receiving mostly poor reviews.\n",
      "Predicted class: Health with probs [0.20330909 0.18147889 0.20359148 0.2022632  0.20935728]\n",
      "Predicted sentiment: Negative with probs [0.36463785 0.29530233 0.34005985]\n",
      "\n",
      "Sentence: Regular exercise and a balanced diet are commonly recommended by health professionals.\n",
      "Predicted class: Health with probs [0.20394741 0.18177487 0.20336343 0.20207384 0.20884047]\n",
      "Predicted sentiment: Negative with probs [0.36422738 0.29488456 0.34088814]\n",
      "\n",
      "Sentence: The software update significantly boosted performance and added exciting new features.\n",
      "Predicted class: Health with probs [0.20350812 0.181802   0.20365089 0.20181848 0.20922044]\n",
      "Predicted sentiment: Negative with probs [0.3635769  0.29502285 0.34140033]\n",
      "\n",
      "Sentence: The athlete was disqualified after a false start in the 100-meter sprint.\n",
      "Predicted class: Health with probs [0.20406656 0.18156421 0.20329557 0.20239978 0.20867385]\n",
      "Predicted sentiment: Negative with probs [0.36438653 0.2945705  0.341043  ]\n",
      "\n",
      "Sentence: The president is scheduled to speak about economic policy at tomorrow’s event.\n",
      "Predicted class: Health with probs [0.20360091 0.18162328 0.20340264 0.20230447 0.20906875]\n",
      "Predicted sentiment: Negative with probs [0.36516452 0.2941147  0.3407208 ]\n",
      "\n",
      "Sentence: The concert was a spectacular event, thrilling fans and selling out in minutes.\n",
      "Predicted class: Health with probs [0.20368722 0.18166278 0.2036465  0.20214108 0.20886245]\n",
      "Predicted sentiment: Negative with probs [0.36413237 0.2944086  0.34145904]\n",
      "\n",
      "Sentence: The study highlighted how poor sleep habits can impair brain function over time.\n",
      "Predicted class: Health with probs [0.20357597 0.18159354 0.20364588 0.201975   0.2092096 ]\n",
      "Predicted sentiment: Negative with probs [0.3637575  0.29453298 0.34170955]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the shared sentence transformer\n",
    "sentence_transformer = SentenceTransformer(\n",
    "    model_name='bert-base-uncased',\n",
    "    pooling_strategy='cls',\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Instantiate the multi-task model\n",
    "mt_model = MultiTaskSentenceTransformer(\n",
    "    sentence_transformer=sentence_transformer,\n",
    "    num_classes=len(cls_classes),  # for main classification task\n",
    "    num_sentiment_classes=len(sentiment_classes),  # for sentiment analysis\n",
    "    hidden_layers=[256, 128],\n",
    "    freeze_transformer=True\n",
    ")\n",
    "\n",
    "# Perform multi-task inference\n",
    "results = multitask_inference(\n",
    "    mt_model,\n",
    "    mt_sentences,\n",
    "    class_names=cls_classes,\n",
    "    sentiment_names=sentiment_classes\n",
    ")\n",
    "\n",
    "# No training -> Just inference\n",
    "for res in results:\n",
    "    print(f\"Sentence: {res['sentence']}\")\n",
    "    print(f\"Predicted class: {res['classification']} with probs {res['classification_probs']}\")\n",
    "    print(f\"Predicted sentiment: {res['sentiment']} with probs {res['sentiment_probs']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56070d1-ee82-453c-854d-7d5976507edd",
   "metadata": {},
   "source": [
    "## Task 4: Training the Multi-Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13063b0-f24a-4c5e-a02e-a3c10ae7407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask_model(model, train_data, train_labels, train_sentiments, \n",
    "                         epochs=3, batch_size=8, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Train a multi-task model with both classification and sentiment analysis\n",
    "    \n",
    "    Args:\n",
    "        model: MultiTaskSentenceTransformer model\n",
    "        train_data: List of training sentences\n",
    "        train_labels: List of training labels (numeric indices)\n",
    "        train_sentiments: List of sentiment labels (numeric indices)\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        device: Device to use for training\n",
    "        \n",
    "    Returns:\n",
    "        Training loss history for both tasks\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    classification_criterion = nn.CrossEntropyLoss()\n",
    "    sentiment_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    labels_tensor = torch.tensor(train_labels, dtype=torch.long, device=device)\n",
    "    sentiments_tensor = torch.tensor(train_sentiments, dtype=torch.long, device=device)\n",
    "    \n",
    "    loss_history = {'classification': [], 'sentiment': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        epoch_class_loss = 0\n",
    "        epoch_sent_loss = 0\n",
    "        \n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            batch_sentences = train_data[i:i+batch_size]\n",
    "            batch_labels = labels_tensor[i:i+batch_size]\n",
    "            batch_sentiments = sentiments_tensor[i:i+batch_size]\n",
    "            \n",
    "            inputs = model.sentence_transformer.tokenizer(\n",
    "                batch_sentences, padding=True, truncation=True, \n",
    "                return_tensors=\"pt\", max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            class_logits, sentiment_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "            \n",
    "            class_loss = classification_criterion(class_logits, batch_labels)\n",
    "            sent_loss = sentiment_criterion(sentiment_logits, batch_sentiments)\n",
    "            total_loss = class_loss + sent_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_class_loss += class_loss.item()\n",
    "            epoch_sent_loss += sent_loss.item()\n",
    "            \n",
    "        avg_class_loss = epoch_class_loss / (len(train_data) // batch_size)\n",
    "        avg_sent_loss = epoch_sent_loss / (len(train_data) // batch_size)\n",
    "        \n",
    "        loss_history['classification'].append(avg_class_loss)\n",
    "        loss_history['sentiment'].append(avg_sent_loss)\n",
    "        \n",
    "        print(f\"Classification loss: {avg_class_loss:.4f} | Sentiment loss: {avg_sent_loss:.4f}\")\n",
    "        \n",
    "    return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9095bf6a-d70a-4e1d-9007-7e5cb546e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multitask_model(model, test_data, test_labels, test_sentiments,\n",
    "                            class_names=None, sentiment_names=None,\n",
    "                            device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate the multi-task model on both classification and sentiment analysis\n",
    "    \n",
    "    Args:\n",
    "        model: MultiTaskSentenceTransformer model\n",
    "        test_data: List of test sentences\n",
    "        test_labels: List of test labels (numeric indices)\n",
    "        test_sentiments: List of test sentiment labels (numeric indices)\n",
    "        class_names: List of class names for classification\n",
    "        sentiment_names: List of sentiment class names\n",
    "        device: Device to use for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (classification_report, sentiment_report)\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_class_preds = []\n",
    "    all_sent_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence in test_data:\n",
    "            # Tokenize single sentence\n",
    "            inputs = model.sentence_transformer.tokenizer(\n",
    "                sentence, padding=True, truncation=True, \n",
    "                return_tensors=\"pt\", max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            class_logits, sentiment_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "            \n",
    "            # Convert to predictions\n",
    "            class_pred = torch.argmax(class_logits, dim=1).item()\n",
    "            sent_pred = torch.argmax(sentiment_logits, dim=1).item()\n",
    "            \n",
    "            all_class_preds.append(class_pred)\n",
    "            all_sent_preds.append(sent_pred)\n",
    "\n",
    "    # Print predictions\n",
    "    print(\"\\nPredictions:\")\n",
    "    for i, (sentence, true_label, true_sent, pred_label, pred_sent) in enumerate(zip(test_data,\n",
    "                                                            [class_names[i] for i in test_labels],\n",
    "                                                            [sentiment_names[i] for i in test_sentiments],\n",
    "                                                            [class_names[i] for i in all_class_preds],\n",
    "                                                            [sentiment_names[i] for i in all_sent_preds])):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"True class: {true_label}\")\n",
    "        print(f\"True sentiment: {true_sent}\")\n",
    "        print(f\"Predicted class: {pred_label}\")\n",
    "        print(f\"Predicted sentiment: {pred_sent}\")\n",
    "        print()\n",
    "    \n",
    "    # Generate classification reports\n",
    "    class_report = classification_report(\n",
    "        test_labels, all_class_preds, target_names=class_names, zero_division=0\n",
    "    )\n",
    "    \n",
    "    sent_report = classification_report(\n",
    "        test_sentiments, all_sent_preds, target_names=sentiment_names, zero_division=0\n",
    "    )\n",
    "    \n",
    "    return class_report, sent_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f09369cd-1f1f-48c6-97a2-4b1cdc29a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multi-task model...\n",
      "Epoch 1/20\n",
      "Classification loss: 2.4031 | Sentiment loss: 1.6439\n",
      "Epoch 2/20\n",
      "Classification loss: 2.3907 | Sentiment loss: 1.6302\n",
      "Epoch 3/20\n",
      "Classification loss: 2.3827 | Sentiment loss: 1.6265\n",
      "Epoch 4/20\n",
      "Classification loss: 2.3759 | Sentiment loss: 1.6183\n",
      "Epoch 5/20\n",
      "Classification loss: 2.3633 | Sentiment loss: 1.6114\n",
      "Epoch 6/20\n",
      "Classification loss: 2.3464 | Sentiment loss: 1.6009\n",
      "Epoch 7/20\n",
      "Classification loss: 2.3326 | Sentiment loss: 1.5852\n",
      "Epoch 8/20\n",
      "Classification loss: 2.3032 | Sentiment loss: 1.5826\n",
      "Epoch 9/20\n",
      "Classification loss: 2.2906 | Sentiment loss: 1.5559\n",
      "Epoch 10/20\n",
      "Classification loss: 2.2567 | Sentiment loss: 1.5363\n",
      "Epoch 11/20\n",
      "Classification loss: 2.2240 | Sentiment loss: 1.4894\n",
      "Epoch 12/20\n",
      "Classification loss: 2.1588 | Sentiment loss: 1.4507\n",
      "Epoch 13/20\n",
      "Classification loss: 2.1475 | Sentiment loss: 1.4748\n",
      "Epoch 14/20\n",
      "Classification loss: 2.0770 | Sentiment loss: 1.3541\n",
      "Epoch 15/20\n",
      "Classification loss: 2.0186 | Sentiment loss: 1.3449\n",
      "Epoch 16/20\n",
      "Classification loss: 1.9498 | Sentiment loss: 1.2701\n",
      "Epoch 17/20\n",
      "Classification loss: 1.8461 | Sentiment loss: 1.1706\n",
      "Epoch 18/20\n",
      "Classification loss: 1.7181 | Sentiment loss: 1.1276\n",
      "Epoch 19/20\n",
      "Classification loss: 1.6950 | Sentiment loss: 1.0411\n",
      "Epoch 20/20\n",
      "Classification loss: 1.6107 | Sentiment loss: 1.0294\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Predictions:\n",
      "Sentence: The new smartphone suffers from poor battery life.\n",
      "True class: Technology\n",
      "True sentiment: Negative\n",
      "Predicted class: Health\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Sentence: The basketball team played their final match of the season, and it ended in a draw.\n",
      "True class: Sports\n",
      "True sentiment: Neutral\n",
      "Predicted class: Sports\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Sentence: The president was praised for introducing a new bill to tackle climate change.\n",
      "True class: Politics\n",
      "True sentiment: Positive\n",
      "Predicted class: Politics\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: The movie was disappointing and received poor reviews from critics and audiences alike.\n",
      "True class: Entertainment\n",
      "True sentiment: Negative\n",
      "Predicted class: Entertainment\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Sentence: Regular exercise and a good sleep schedule are essential for maintaining good health.\n",
      "True class: Health\n",
      "True sentiment: Neutral\n",
      "Predicted class: Health\n",
      "Predicted sentiment: Neutral\n",
      "\n",
      "Sentence: The software update is amazing!\n",
      "True class: Technology\n",
      "True sentiment: Positive\n",
      "Predicted class: Entertainment\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: I was sad to see the athlete from Jamaica named Usain Bolt getting disqualified.\n",
      "True class: Sports\n",
      "True sentiment: Negative\n",
      "Predicted class: Entertainment\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: The president will deliver a speech on foreign policy.\n",
      "True class: Politics\n",
      "True sentiment: Neutral\n",
      "Predicted class: Politics\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: The tickets for the basketball game sold out within minutes and the game was thrilling to watch.\n",
      "True class: Entertainment\n",
      "True sentiment: Positive\n",
      "Predicted class: Entertainment\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: The study found a strong correlation between a poor diet and low lifespan.\n",
      "True class: Health\n",
      "True sentiment: Negative\n",
      "Predicted class: Health\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Technology       0.00      0.00      0.00         2\n",
      "       Sports       1.00      0.50      0.67         2\n",
      "     Politics       1.00      1.00      1.00         2\n",
      "Entertainment       0.50      1.00      0.67         2\n",
      "       Health       0.67      1.00      0.80         2\n",
      "\n",
      "     accuracy                           0.70        10\n",
      "    macro avg       0.63      0.70      0.63        10\n",
      " weighted avg       0.63      0.70      0.63        10\n",
      "\n",
      "\n",
      "Sentiment Analysis Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.75      0.75         4\n",
      "     Neutral       1.00      0.33      0.50         3\n",
      "    Positive       0.60      1.00      0.75         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.78      0.69      0.67        10\n",
      "weighted avg       0.78      0.70      0.68        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_test_multitask():\n",
    "    # Classification task setup\n",
    "    class_names = [\"Technology\", \"Sports\", \"Politics\", \"Entertainment\", \"Health\"]\n",
    "    sentiment_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    \n",
    "    # Sample data with both tasks\n",
    "    train_sentences = [\n",
    "        # Technology - Negative\n",
    "        \"The new smartphone suffers from overheating issues and frequent app crashes.\",\n",
    "        # Sports - Neutral\n",
    "        \"The team played their final match of the season, ending in a 1-1 draw.\",\n",
    "        # Politics - Positive\n",
    "        \"The senator was praised for introducing an innovative bill to tackle climate change.\",\n",
    "        # Entertainment - Negative\n",
    "        \"The movie failed to impress critics, receiving mostly poor reviews.\",\n",
    "        # Health - Neutral\n",
    "        \"Regular exercise and a balanced diet are commonly recommended by health professionals.\",\n",
    "        # Technology - Positive\n",
    "        \"The software update significantly boosted performance and added exciting new features.\",\n",
    "        # Sports - Negative\n",
    "        \"The athlete was disqualified after a false start in the 100-meter sprint.\",\n",
    "        # Politics - Neutral\n",
    "        \"The president is scheduled to speak about economic policy at tomorrow’s event.\",\n",
    "        # Entertainment - Positive\n",
    "        \"The concert was a spectacular event, thrilling fans and selling out in minutes.\",\n",
    "        # Health - Negative\n",
    "        \"The study highlighted how poor sleep habits can impair brain function over time.\"\n",
    "    ]\n",
    "    \n",
    "    test_sentences = [\n",
    "        \"The new smartphone suffers from poor battery life.\",\n",
    "        \"The basketball team played their final match of the season, and it ended in a draw.\",\n",
    "        \"The president was praised for introducing a new bill to tackle climate change.\",\n",
    "        \"The movie was disappointing and received poor reviews from critics and audiences alike.\",\n",
    "        \"Regular exercise and a good sleep schedule are essential for maintaining good health.\",\n",
    "        \"The software update is amazing!\",\n",
    "        \"I was sad to see the athlete from Jamaica named Usain Bolt getting disqualified.\",\n",
    "        \"The president will deliver a speech on foreign policy.\",\n",
    "        \"The tickets for the basketball game sold out within minutes and the game was thrilling to watch.\",\n",
    "        \"The study found a strong correlation between a poor diet and low lifespan.\"\n",
    "    ]\n",
    "    \n",
    "    # Train Labels\n",
    "    train_cls_labels = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "    train_sentiment_labels = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]\n",
    "    \n",
    "    # Test Labels\n",
    "    test_cls_labels = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "    test_sentiment_labels = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]\n",
    "    \n",
    "    # Initialize the model\n",
    "    sentence_transformer = SentenceTransformer(\n",
    "        model_name='bert-base-uncased',\n",
    "        pooling_strategy='cls',\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    model = MultiTaskSentenceTransformer(\n",
    "        sentence_transformer=sentence_transformer,\n",
    "        num_classes=len(class_names),\n",
    "        num_sentiment_classes=len(sentiment_names),\n",
    "        hidden_layers=[256, 128],\n",
    "        freeze_transformer=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training multi-task model...\")\n",
    "    train_multitask_model(\n",
    "        model, train_sentences, train_cls_labels, train_sentiment_labels,\n",
    "        epochs=20, batch_size=4\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluation:\")\n",
    "    class_report, sent_report = evaluate_multitask_model(\n",
    "        model, test_sentences, test_cls_labels, test_sentiment_labels,\n",
    "        class_names, sentiment_names\n",
    "    )\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    print(\"\\nSentiment Analysis Report:\")\n",
    "    print(sent_report)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_test_multitask()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c88dbd1",
   "metadata": {},
   "source": [
    "### Further multi-task training modifications discussed in the write-up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f51fbe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

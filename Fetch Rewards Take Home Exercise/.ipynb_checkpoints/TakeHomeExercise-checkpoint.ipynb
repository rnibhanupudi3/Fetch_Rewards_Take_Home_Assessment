{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8032a236-c689-4df6-8026-29b8a13ecac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installs\n",
    "\n",
    "! pip install --quiet torch\n",
    "! pip install --quiet numpy\n",
    "! pip install --quiet transformers\n",
    "# ! pip install --quiet hf_xet\n",
    "! pip install --quiet scikit-learn\n",
    "# ! pip install --user --quiet ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc06b40-20b9-4c42-895f-7c2dafc1902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88199f-d359-46b6-9af9-d7a86de08e7e",
   "metadata": {},
   "source": [
    "## Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55838515-c471-4fde-a4cc-a74ae4279428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\"\n",
    "    Compute mean pooling of token embeddings, ignoring padding tokens.\n",
    "    \n",
    "    Args:\n",
    "        token_embeddings: Tensor of shape [batch_size, seq_len, hidden_size]\n",
    "        attention_mask: Tensor of shape [batch_size, seq_len] with 1s for tokens and 0s for padding\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape [batch_size, hidden_size] with sentence embeddings\n",
    "    \"\"\"\n",
    "    # Convert attention mask to float for multiplication\n",
    "    mask = attention_mask.unsqueeze(-1).float()\n",
    "    \n",
    "    # Sum the embeddings of real tokens (multiply by mask to zero out padding)\n",
    "    sum_embeddings = torch.sum(token_embeddings * mask, dim=1)\n",
    "    \n",
    "    # Count the number of real tokens per sentence\n",
    "    token_counts = torch.sum(attention_mask, dim=1, keepdim=True).float()\n",
    "    \n",
    "    # Compute mean by dividing sum by count\n",
    "    sentence_embeddings = sum_embeddings / token_counts\n",
    "    \n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def max_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\"\n",
    "    Compute max pooling of token embeddings, ignoring padding tokens.\n",
    "    \n",
    "    Args:\n",
    "        token_embeddings: Tensor of shape [batch_size, seq_len, hidden_size]\n",
    "        attention_mask: Tensor of shape [batch_size, seq_len] with 1s for tokens and 0s for padding\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape [batch_size, hidden_size] with sentence embeddings\n",
    "    \"\"\"\n",
    "    # Create a mask for padding tokens (0s for padding, 1s for real tokens)\n",
    "    mask = attention_mask.unsqueeze(-1).expand_as(token_embeddings).float()\n",
    "    \n",
    "    # Replace padding token embeddings with large negative values\n",
    "    # This ensures they won't be selected during max operation\n",
    "    masked_embeddings = token_embeddings.clone()\n",
    "    masked_embeddings[mask == 0] = -1e9\n",
    "    \n",
    "    # Take max over sequence dimension (dim=1)\n",
    "    sentence_embeddings = torch.max(masked_embeddings, dim=1)[0]\n",
    "    \n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8c1a93-fba2-4a97-b0e9-7f91225956c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768, pooling_strategy='mean', normalize_embeddings=True):\n",
    "        \"\"\"\n",
    "        Initialize the Sentence Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Pre-trained transformer model name\n",
    "            embedding_dim: Dimension of embeddings from the transformer model\n",
    "            pooling_strategy: Strategy to pool token embeddings into sentence embedding\n",
    "            projection_dim: If provided, project embeddings to this dimension\n",
    "            normalize_embeddings: Whether to normalize final embeddings\n",
    "        \"\"\"\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        \n",
    "        # Load pre-trained transformer model\n",
    "        self.transformer = BertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Configure pooling strategy\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "            \n",
    "        # Whether to normalize final embeddings\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        # Get transformer outputs\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get embeddings from the transformer output\n",
    "        token_embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Apply pooling strategy\n",
    "        if self.pooling_strategy == 'cls':\n",
    "            # Use [CLS] token embedding\n",
    "            sentence_embedding = token_embeddings[:, 0, :]\n",
    "        elif self.pooling_strategy == 'mean':\n",
    "            # Mean pooling - take mean of all BERT embeddings for a sentence\n",
    "            sentence_embedding = mean_pooling(token_embeddings, attention_mask)\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            # Max pooling - take max of all BERT embeddings for a sentence\n",
    "            sentence_embedding = max_pooling(token_embeddings, attention_mask)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling strategy: {self.pooling_strategy}\")\n",
    "        \n",
    "        # Normalize embeddings so that similar sentences can be compared via cosine similarity\n",
    "        if self.normalize_embeddings:\n",
    "            sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)\n",
    "            \n",
    "        return sentence_embedding\n",
    "\n",
    "    \n",
    "    def encode_single(self, sentence, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Encode a single sentence into an embedding\n",
    "        \n",
    "        Args:\n",
    "            sentence: String sentence to encode\n",
    "            device: Device to use for computation\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of sentence embedding\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        \n",
    "        # Tokenize the sentence\n",
    "        inputs = self.tokenizer(sentence, padding=True, truncation=True, \n",
    "                               return_tensors=\"pt\", max_length=512)\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = self.forward(inputs['input_ids'], inputs['attention_mask'])\n",
    "        \n",
    "        # Move embedding to CPU and convert to numpy\n",
    "        return embedding.detach().cpu().numpy()[0]  # Get the first (and only) item in batch\n",
    "\n",
    "\n",
    "    def encode(self, sentences, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Encode a list of sentences into embeddings, processing one sentence at a time\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences to encode\n",
    "            device: Device to use for computation\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of sentence embeddings\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process sentences one at a time\n",
    "        for sentence in sentences:\n",
    "            embedding = self.encode_single(sentence, device)\n",
    "            all_embeddings.append(embedding)\n",
    "        \n",
    "        # Stack all embeddings\n",
    "        all_embeddings = np.vstack(all_embeddings)\n",
    "        \n",
    "        return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd981ff6-7fe9-4bdd-986c-de9ec0865f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: BERT-base with mean pooling\n",
      "Embedding shape: (6, 768)\n",
      "\n",
      "Sample embeddings (first 5 dimensions):\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [-0.0015162  -0.00784877  0.0059078   0.00047336  0.04285677]...\n",
      "\n",
      "Sentence: I love learning about natural language processing.\n",
      "Embedding: [ 0.04697719  0.04027892 -0.01634424 -0.00495687  0.02874718]...\n",
      "\n",
      "Sentence: Sentence transformers are useful for many NLP tasks.\n",
      "Embedding: [ 0.00526725 -0.03937274 -0.00760075  0.00709967  0.01425832]...\n",
      "\n",
      "Sentence: This is a test sentence to evaluate the model.\n",
      "Embedding: [ 0.0037509  -0.02287795 -0.01076239 -0.01012837 -0.01170714]...\n",
      "\n",
      "Sentence: The weather is beautiful today, perfect for a walk outside.\n",
      "Embedding: [ 0.04992568 -0.0286326   0.01436006  0.02499489  0.01484542]...\n",
      "\n",
      "Sentence: Sentence embeddings for each sentence are of the size 768\n",
      "Embedding: [-0.01830662  0.00069706  0.00832807 -0.01535794 -0.00211742]...\n",
      "\n",
      "\n",
      "Testing: BERT-base with CLS pooling\n",
      "Embedding shape: (6, 768)\n",
      "\n",
      "Sample embeddings (first 5 dimensions):\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [-0.02430748  0.01529824 -0.02041111 -0.01266661  0.00320098]...\n",
      "\n",
      "Sentence: I love learning about natural language processing.\n",
      "Embedding: [ 0.00565162  0.00544924 -0.02073738 -0.02403913 -0.02869998]...\n",
      "\n",
      "Sentence: Sentence transformers are useful for many NLP tasks.\n",
      "Embedding: [-0.02287435 -0.02190962 -0.01619019 -0.01621452 -0.03725113]...\n",
      "\n",
      "Sentence: This is a test sentence to evaluate the model.\n",
      "Embedding: [-0.0158417  -0.01448572 -0.01255657 -0.01273945 -0.03262843]...\n",
      "\n",
      "Sentence: The weather is beautiful today, perfect for a walk outside.\n",
      "Embedding: [ 0.02903327 -0.00404882 -0.0175745  -0.01611305 -0.01414119]...\n",
      "\n",
      "Sentence: Sentence embeddings for each sentence are of the size 768\n",
      "Embedding: [-0.02316322  0.00906089  0.00740391 -0.01824841 -0.01481495]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model with sample sentences\n",
    "def main():\n",
    "    # Initialize model with different configurations\n",
    "    models = {\n",
    "        \"BERT-base with mean pooling\": SentenceTransformer(\n",
    "            model_name='bert-base-uncased', \n",
    "            pooling_strategy='mean',\n",
    "            normalize_embeddings=True\n",
    "        ),\n",
    "        \"BERT-base with CLS pooling\": SentenceTransformer(\n",
    "            model_name='bert-base-uncased', \n",
    "            pooling_strategy='cls',\n",
    "            normalize_embeddings=True\n",
    "        ),\n",
    "    }\n",
    "        \n",
    "    # Sample sentences\n",
    "    sample_sentences = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"I love learning about natural language processing.\",\n",
    "        \"Sentence transformers are useful for many NLP tasks.\",\n",
    "        \"This is a test sentence to evaluate the model.\",\n",
    "        \"The weather is beautiful today, perfect for a walk outside.\",\n",
    "        \"Sentence embeddings for each sentence are of the size 768\"\n",
    "    ]\n",
    "    \n",
    "    # Test each model configuration\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTesting: {model_name}\")\n",
    "        \n",
    "        # Encode sentences\n",
    "        for se\n",
    "        embeddings = model.encode(sample_sentences)\n",
    "        \n",
    "        # Print embedding dimensions\n",
    "        print(f\"Embedding shape: {embeddings.shape}, Number of sentences: \")\n",
    "        \n",
    "        print(\"\\nSample embeddings (first 5 dimensions):\")\n",
    "        for i, sentence in enumerate(sample_sentences):\n",
    "            print(f\"Sentence: {sentence}\")\n",
    "            print(f\"Embedding: {embeddings[i][:5]}...\")\n",
    "            print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd657719-0651-49ca-9601-d6a4ca44aeda",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb44bf-6af3-489e-b968-92ff14e896d9",
   "metadata": {},
   "source": [
    "### Task A: Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d37c4e0-eae1-4b04-8c57-297ac1c718eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, sentence_transformer, num_classes, hidden_layers=None, freeze_transformer=True):\n",
    "        \"\"\"\n",
    "        Initialize a classifier that uses sentence embeddings.\n",
    "        \n",
    "        Args:\n",
    "            sentence_transformer: SentenceTransformer model for creating embeddings\n",
    "            num_classes: Number of output classes for classification\n",
    "            hidden_layers: List of hidden layer sizes (if None, only a single classification layer is used)\n",
    "            freeze_transformer: Whether to freeze the transformer part during training\n",
    "        \"\"\"\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        \n",
    "        self.sentence_transformer = sentence_transformer\n",
    "        \n",
    "        # Freeze transformer parameters if specified\n",
    "        if freeze_transformer:\n",
    "            for param in self.sentence_transformer.transformer.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Get embedding dimension\n",
    "        embedding_dim = self.sentence_transformer.embedding_dim\n",
    "        \n",
    "        # Create classifier layers\n",
    "        if hidden_layers is None:\n",
    "            # Simple linear classifier\n",
    "            self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "        else:\n",
    "            # Multi-layer classifier\n",
    "            layers = []\n",
    "            input_dim = embedding_dim\n",
    "            \n",
    "            # Add hidden layers\n",
    "            for hidden_dim in hidden_layers:\n",
    "                layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(0.1))\n",
    "                input_dim = hidden_dim\n",
    "                \n",
    "            # Add output layer\n",
    "            layers.append(nn.Linear(input_dim, num_classes))\n",
    "            \n",
    "            self.classifier = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        # Get sentence embeddings\n",
    "        embeddings = self.sentence_transformer(input_ids, attention_mask)\n",
    "        \n",
    "        # Apply classifier\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def classify(self, sentences, class_names=None, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Classify a list of sentences\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences to classify\n",
    "            class_names: List of class names (if None, returns class indices)\n",
    "            device: Device to use for computation\n",
    "            \n",
    "        Returns:\n",
    "            List of predicted classes\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        # Process sentences one at a time\n",
    "        for sentence in sentences:\n",
    "            # Tokenize the sentence\n",
    "            inputs = self.sentence_transformer.tokenizer(sentence, padding=True, truncation=True, \n",
    "                                                        return_tensors=\"pt\", max_length=512)\n",
    "            \n",
    "            # Move inputs to device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                logits = self.forward(inputs['input_ids'], inputs['attention_mask'])\n",
    "                prediction = torch.argmax(logits, dim=1).item()\n",
    "            \n",
    "            # Convert to class name if provided\n",
    "            if class_names is not None:\n",
    "                prediction = class_names[prediction]\n",
    "                \n",
    "            all_predictions.append(prediction)\n",
    "        \n",
    "        return all_predictions\n",
    "    \n",
    "    def pred_probs(self, sentences, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Get class probabilities for a list of sentences\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences to classify\n",
    "            device: Device to use for computation\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array of class probabilities\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        \n",
    "        all_probs = []\n",
    "        \n",
    "        # Process sentences one at a time\n",
    "        for sentence in sentences:\n",
    "            # Tokenize the sentence\n",
    "            inputs = self.sentence_transformer.tokenizer(sentence, padding=True, truncation=True, \n",
    "                                                        return_tensors=\"pt\", max_length=512)\n",
    "            \n",
    "            # Move inputs to device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                logits = self.forward(inputs['input_ids'], inputs['attention_mask'])\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            all_probs.append(probs.cpu().numpy()[0])\n",
    "        \n",
    "        return np.array(all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669357ca-dc4d-4f66-aa61-20254eca53bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting classes...\n",
      "\n",
      "Predictions:\n",
      "Sentence: The new smartphone features an AI-powered camera and improved battery life.\n",
      "True class: Technology\n",
      "Predicted class: Health\n",
      "\n",
      "Sentence: The team scored a last-minute goal to win the championship.\n",
      "True class: Sports\n",
      "Predicted class: Health\n",
      "\n",
      "\n",
      "Class probabilities:\n",
      "Sentence: The new smartphone features an AI-powered camera and improved battery life.\n",
      "  Technology: 0.1974\n",
      "  Sports: 0.2047\n",
      "  Politics: 0.1998\n",
      "  Entertainment: 0.1809\n",
      "  Health: 0.2172\n",
      "\n",
      "Sentence: The team scored a last-minute goal to win the championship.\n",
      "  Technology: 0.1972\n",
      "  Sports: 0.2041\n",
      "  Politics: 0.2004\n",
      "  Entertainment: 0.1811\n",
      "  Health: 0.2172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sentence_classification():\n",
    "    # Define a sample classification task\n",
    "    class_names = [\"Technology\", \"Sports\", \"Politics\", \"Entertainment\", \"Health\"]\n",
    "    \n",
    "    # Sample data\n",
    "    sample_sentences = [\n",
    "        \"The new smartphone features an AI-powered camera and improved battery life.\",\n",
    "        \"The team scored a last-minute goal to win the championship.\",\n",
    "        \"The senator proposed a new bill to address climate change.\",\n",
    "        \"The movie received excellent reviews from critics and audiences alike.\",\n",
    "        \"Regular exercise and a balanced diet are essential for maintaining good health.\",\n",
    "        \"The software update includes security patches and performance improvements.\",\n",
    "        \"The athlete broke the world record in the 100-meter sprint.\",\n",
    "        \"The president will deliver a speech on economic policy tomorrow.\",\n",
    "        \"The concert tickets sold out within minutes of going on sale.\",\n",
    "        \"The study found a strong correlation between sleep quality and cognitive function.\"\n",
    "    ]\n",
    "    \n",
    "    # Sample labels (indices of class_names)\n",
    "    sample_labels = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "    \n",
    "    # Initialize the sentence transformer\n",
    "    sentence_transformer = SentenceTransformer(\n",
    "        model_name='bert-base-uncased',\n",
    "        pooling_strategy='cls',\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    classifier = SentenceClassifier(\n",
    "        sentence_transformer=sentence_transformer,\n",
    "        num_classes=len(class_names),\n",
    "        hidden_layers=[256, 128],  # Two hidden layers\n",
    "        freeze_transformer=True    # Freeze transformer weights\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\nPredicting classes...\")\n",
    "    predictions = classifier.classify(sample_sentences[:2], class_names=class_names)\n",
    "    \n",
    "    # Print predictions\n",
    "    print(\"\\nPredictions:\")\n",
    "    for i, (sentence, true_class, pred_class) in enumerate(zip(sample_sentences[:2], \n",
    "                                                            [class_names[i] for i in sample_labels[:2]], \n",
    "                                                            predictions)):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"True class: {true_class}\")\n",
    "        print(f\"Predicted class: {pred_class}\")\n",
    "        print()\n",
    "    \n",
    "    # Get classification probabilities\n",
    "    print(\"\\nClass probabilities:\")\n",
    "    probs = classifier.pred_probs(sample_sentences[:2])  # Just show first two for brevity\n",
    "    for i, (sentence, prob) in enumerate(zip(sample_sentences[:2], probs)):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        for j, class_name in enumerate(class_names):\n",
    "            print(f\"  {class_name}: {prob[j]:.4f}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentence_classification()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d42d21-b0cb-4a5a-ad24-12753be40422",
   "metadata": {},
   "source": [
    "### Task B: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a36da9-8766-4f02-b2b9-4c9ddf68b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting classes...\n",
      "\n",
      "Predictions:\n",
      "Sentence: This product is absolutely terrible and completely failed to meet my expectations.\n",
      "True class: Negative\n",
      "Predicted class: Negative\n",
      "\n",
      "Sentence: The service was okay, but nothing special to write home about.\n",
      "True class: Neutral\n",
      "Predicted class: Negative\n",
      "\n",
      "\n",
      "Class probabilities:\n",
      "Sentence: This product is absolutely terrible and completely failed to meet my expectations.\n",
      "  Negative: 0.3520\n",
      "  Neutral: 0.3439\n",
      "  Positive: 0.3041\n",
      "\n",
      "Sentence: The service was okay, but nothing special to write home about.\n",
      "  Negative: 0.3517\n",
      "  Neutral: 0.3444\n",
      "  Positive: 0.3039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sentiment_analysis():\n",
    "    # Define sentiment analysis classes\n",
    "    class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    \n",
    "    # Sample data with varied sentiment\n",
    "    sample_sentences = [\n",
    "        \"This product is absolutely terrible and completely failed to meet my expectations.\",\n",
    "        \"The service was okay, but nothing special to write home about.\",\n",
    "        \"I'm absolutely thrilled with my purchase! Best decision I've made all year.\",\n",
    "        \"The customer support experience was frustrating and completely unhelpful.\",\n",
    "        \"The movie was fairly standard, pretty much what you'd expect from this genre.\",\n",
    "        \"The restaurant exceeded all my expectations - incredible food and impeccable service!\",\n",
    "        \"I regret spending money on this disappointing product.\",\n",
    "        \"The conference covered the topics that were advertised but lacked depth.\",\n",
    "        \"The hotel staff went above and beyond to make our stay memorable and special.\",\n",
    "        \"This app constantly crashes and has wasted hours of my time.\",\n",
    "        \"The book contains useful information presented in a straightforward manner.\",\n",
    "        \"The concert was an amazing experience that I'll remember for years to come.\"\n",
    "    ]\n",
    "    \n",
    "    # Sample labels (0=Negative, 1=Neutral, 2=Positive)\n",
    "    sample_labels = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "    \n",
    "    # Initialize the sentence transformer\n",
    "    sentence_transformer = SentenceTransformer(\n",
    "        model_name='bert-base-uncased',\n",
    "        pooling_strategy='cls',\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    classifier = SentenceClassifier(\n",
    "        sentence_transformer=sentence_transformer,\n",
    "        num_classes=len(class_names),\n",
    "        hidden_layers=[256, 128],  # Two hidden layers\n",
    "        freeze_transformer=True    # Freeze transformer weights\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\nPredicting classes...\")\n",
    "    predictions = classifier.classify(sample_sentences[:2], class_names=class_names)\n",
    "    \n",
    "    # Print predictions\n",
    "    print(\"\\nPredictions:\")\n",
    "    for i, (sentence, true_class, pred_class) in enumerate(zip(sample_sentences[:2], \n",
    "                                                            [class_names[i] for i in sample_labels[:2]], \n",
    "                                                            predictions)):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"True class: {true_class}\")\n",
    "        print(f\"Predicted class: {pred_class}\")\n",
    "        print()\n",
    "    \n",
    "    # Get classification probabilities\n",
    "    print(\"\\nClass probabilities:\")\n",
    "    probs = classifier.pred_probs(sample_sentences[:2])  # Just show first two for brevity\n",
    "    for i, (sentence, prob) in enumerate(zip(sample_sentences[:2], probs)):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        for j, class_name in enumerate(class_names):\n",
    "            print(f\"  {class_name}: {prob[j]:.4f}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentiment_analysis()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a548b-afbd-476c-a514-5239d988dcca",
   "metadata": {},
   "source": [
    "### Multi-Task Learning Expansion (Both Classification and Sentiment Analysis in one model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3e66415-3202-4428-9557-79a3c745162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample classification task\n",
    "cls_classes = [\"Technology\", \"Sports\", \"Politics\", \"Entertainment\", \"Health\"]\n",
    "sentiment_classes = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "mt_sentences = [\n",
    "    # Technology - Negative\n",
    "    \"The new smartphone suffers from overheating issues and frequent app crashes.\",\n",
    "    # Sports - Neutral\n",
    "    \"The team played their final match of the season, ending in a 1-1 draw.\",\n",
    "    # Politics - Positive\n",
    "    \"The senator was praised for introducing an innovative bill to tackle climate change.\",\n",
    "    # Entertainment - Negative\n",
    "    \"The movie failed to impress critics, receiving mostly poor reviews.\",\n",
    "    # Health - Neutral\n",
    "    \"Regular exercise and a balanced diet are commonly recommended by health professionals.\",\n",
    "    # Technology - Positive\n",
    "    \"The software update significantly boosted performance and added exciting new features.\",\n",
    "    # Sports - Negative\n",
    "    \"The athlete was disqualified after a false start in the 100-meter sprint.\",\n",
    "    # Politics - Neutral\n",
    "    \"The president is scheduled to speak about economic policy at tomorrow’s event.\",\n",
    "    # Entertainment - Positive\n",
    "    \"The concert was a spectacular event, thrilling fans and selling out in minutes.\",\n",
    "    # Health - Negative\n",
    "    \"The study highlighted how poor sleep habits can impair brain function over time.\"\n",
    "]\n",
    "\n",
    "# Sample labels (indices of class_names)\n",
    "cls_labels = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "sentiment_labels = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4c8137f-5296-4f31-9834-cd767b1020d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    def __init__(self, sentence_transformer, num_classes, num_sentiment_classes=3, hidden_layers=None, freeze_transformer=True):\n",
    "        super(MultiTaskSentenceTransformer, self).__init__()\n",
    "        self.sentence_transformer = sentence_transformer\n",
    "        if freeze_transformer:\n",
    "            for param in self.sentence_transformer.transformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        embedding_dim = self.sentence_transformer.embedding_dim\n",
    "\n",
    "        # Classification head\n",
    "        if hidden_layers is None:\n",
    "            self.classification_head = nn.Linear(embedding_dim, num_classes)\n",
    "            self.sentiment_head = nn.Linear(embedding_dim, num_sentiment_classes)\n",
    "        else:\n",
    "            layers = []\n",
    "            input_dim = embedding_dim\n",
    "            for hidden_dim in hidden_layers:\n",
    "                layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(0.1))\n",
    "                input_dim = hidden_dim\n",
    "            cls_head = layers + [nn.Linear(input_dim, num_classes)]\n",
    "            sent_head = layers + [nn.Linear(input_dim, num_sentiment_classes)]\n",
    "            self.classification_head = nn.Sequential(*cls_head)\n",
    "            self.sentiment_head = nn.Sequential(*sent_head)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embeddings = self.sentence_transformer(input_ids, attention_mask)\n",
    "        class_logits = self.classification_head(embeddings)\n",
    "        sentiment_logits = self.sentiment_head(embeddings)\n",
    "        return class_logits, sentiment_logits\n",
    "\n",
    "# Unified inference function\n",
    "def multitask_inference(model, sentences, class_names=None, sentiment_names=None, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        inputs = model.sentence_transformer.tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            class_logits, sentiment_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "            # Get predicted class\n",
    "            class_pred_idx = torch.argmax(class_logits, dim=1).item()\n",
    "            sentiment_pred_idx = torch.argmax(sentiment_logits, dim=1).item()\n",
    "\n",
    "            # Map to class names if provided\n",
    "            class_pred = class_names[class_pred_idx] if class_names is not None else class_pred_idx\n",
    "            sentiment_pred = sentiment_names[sentiment_pred_idx] if sentiment_names is not None else sentiment_pred_idx\n",
    "\n",
    "        predictions.append({\n",
    "            'sentence': sentence,\n",
    "            'classification': class_pred,\n",
    "            'sentiment': sentiment_pred,\n",
    "            'classification_probs': F.softmax(class_logits, dim=1).cpu().numpy()[0],\n",
    "            'sentiment_probs': F.softmax(sentiment_logits, dim=1).cpu().numpy()[0]\n",
    "        })\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73df3984-ed8c-499c-aa59-f492706d295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The new smartphone suffers from overheating issues and frequent app crashes.\n",
      "Predicted class: Technology with probs [0.2060228  0.20441142 0.19068728 0.20131111 0.19756739]\n",
      "Predicted sentiment: Neutral with probs [0.29558885 0.35714564 0.34726557]\n",
      "\n",
      "Sentence: The team played their final match of the season, ending in a 1-1 draw.\n",
      "Predicted class: Technology with probs [0.20640352 0.2046291  0.18990968 0.20117389 0.19788389]\n",
      "Predicted sentiment: Neutral with probs [0.2959986 0.3568129 0.3471885]\n",
      "\n",
      "Sentence: The senator was praised for introducing an innovative bill to tackle climate change.\n",
      "Predicted class: Technology with probs [0.20596606 0.20517409 0.19099422 0.20061123 0.19725442]\n",
      "Predicted sentiment: Neutral with probs [0.2965526  0.35689047 0.34655687]\n",
      "\n",
      "Sentence: The movie failed to impress critics, receiving mostly poor reviews.\n",
      "Predicted class: Technology with probs [0.2062286  0.20509274 0.19003966 0.20131613 0.19732285]\n",
      "Predicted sentiment: Neutral with probs [0.2961092  0.35679734 0.34709346]\n",
      "\n",
      "Sentence: Regular exercise and a balanced diet are commonly recommended by health professionals.\n",
      "Predicted class: Technology with probs [0.20638008 0.20444906 0.19000566 0.20132908 0.19783612]\n",
      "Predicted sentiment: Neutral with probs [0.29552084 0.3572097  0.34726942]\n",
      "\n",
      "Sentence: The software update significantly boosted performance and added exciting new features.\n",
      "Predicted class: Technology with probs [0.20632489 0.20455112 0.19036642 0.20127885 0.19747873]\n",
      "Predicted sentiment: Neutral with probs [0.2955221  0.35716927 0.34730867]\n",
      "\n",
      "Sentence: The athlete was disqualified after a false start in the 100-meter sprint.\n",
      "Predicted class: Technology with probs [0.20560761 0.20520775 0.19022925 0.2010302  0.19792517]\n",
      "Predicted sentiment: Neutral with probs [0.29590756 0.35742563 0.34666687]\n",
      "\n",
      "Sentence: The president is scheduled to speak about economic policy at tomorrow’s event.\n",
      "Predicted class: Technology with probs [0.20600244 0.204776   0.19054712 0.20100561 0.19766882]\n",
      "Predicted sentiment: Neutral with probs [0.29570967 0.3568905  0.34739983]\n",
      "\n",
      "Sentence: The concert was a spectacular event, thrilling fans and selling out in minutes.\n",
      "Predicted class: Technology with probs [0.20600756 0.2049143  0.19048533 0.20127513 0.19731763]\n",
      "Predicted sentiment: Neutral with probs [0.29523376 0.3575927  0.3471735 ]\n",
      "\n",
      "Sentence: The study highlighted how poor sleep habits can impair brain function over time.\n",
      "Predicted class: Technology with probs [0.20605019 0.20475462 0.19029534 0.20120138 0.19769852]\n",
      "Predicted sentiment: Neutral with probs [0.2953937  0.35677117 0.34783515]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the shared sentence transformer\n",
    "sentence_transformer = SentenceTransformer(\n",
    "    model_name='bert-base-uncased',\n",
    "    pooling_strategy='cls',\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Instantiate the multi-task model\n",
    "mt_model = MultiTaskSentenceTransformer(\n",
    "    sentence_transformer=sentence_transformer,\n",
    "    num_classes=len(cls_classes),  # for main classification task\n",
    "    num_sentiment_classes=len(sentiment_classes),  # for sentiment analysis\n",
    "    hidden_layers=[256, 128],\n",
    "    freeze_transformer=True\n",
    ")\n",
    "\n",
    "# Perform multi-task inference\n",
    "# sentences = [\"The new phone is amazing!\", \"I am not happy with the service.\"]\n",
    "results = multitask_inference(\n",
    "    mt_model,\n",
    "    mt_sentences,\n",
    "    class_names=cls_classes,\n",
    "    sentiment_names=sentiment_classes\n",
    ")\n",
    "\n",
    "# No training -> Just inference\n",
    "for res in results:\n",
    "    print(f\"Sentence: {res['sentence']}\")\n",
    "    print(f\"Predicted class: {res['classification']} with probs {res['classification_probs']}\")\n",
    "    print(f\"Predicted sentiment: {res['sentiment']} with probs {res['sentiment_probs']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56070d1-ee82-453c-854d-7d5976507edd",
   "metadata": {},
   "source": [
    "## Task 4: Training the Multi-Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f13063b0-f24a-4c5e-a02e-a3c10ae7407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask_model(model, train_data, train_labels, train_sentiments, \n",
    "                         epochs=3, batch_size=8, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Train a multi-task model with both classification and sentiment analysis\n",
    "    \n",
    "    Args:\n",
    "        model: MultiTaskSentenceTransformer model\n",
    "        train_data: List of training sentences\n",
    "        train_labels: List of training labels (numeric indices)\n",
    "        train_sentiments: List of sentiment labels (numeric indices)\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        device: Device to use for training\n",
    "        \n",
    "    Returns:\n",
    "        Training loss history for both tasks\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    classification_criterion = nn.CrossEntropyLoss()\n",
    "    sentiment_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    labels_tensor = torch.tensor(train_labels, dtype=torch.long, device=device)\n",
    "    sentiments_tensor = torch.tensor(train_sentiments, dtype=torch.long, device=device)\n",
    "    \n",
    "    loss_history = {'classification': [], 'sentiment': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        epoch_class_loss = 0\n",
    "        epoch_sent_loss = 0\n",
    "        \n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            batch_sentences = train_data[i:i+batch_size]\n",
    "            batch_labels = labels_tensor[i:i+batch_size]\n",
    "            batch_sentiments = sentiments_tensor[i:i+batch_size]\n",
    "            \n",
    "            inputs = model.sentence_transformer.tokenizer(\n",
    "                batch_sentences, padding=True, truncation=True, \n",
    "                return_tensors=\"pt\", max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            class_logits, sentiment_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "            \n",
    "            class_loss = classification_criterion(class_logits, batch_labels)\n",
    "            sent_loss = sentiment_criterion(sentiment_logits, batch_sentiments)\n",
    "            total_loss = class_loss + sent_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_class_loss += class_loss.item()\n",
    "            epoch_sent_loss += sent_loss.item()\n",
    "            \n",
    "        avg_class_loss = epoch_class_loss / (len(train_data) // batch_size)\n",
    "        avg_sent_loss = epoch_sent_loss / (len(train_data) // batch_size)\n",
    "        \n",
    "        loss_history['classification'].append(avg_class_loss)\n",
    "        loss_history['sentiment'].append(avg_sent_loss)\n",
    "        \n",
    "        print(f\"Classification loss: {avg_class_loss:.4f} | Sentiment loss: {avg_sent_loss:.4f}\")\n",
    "        \n",
    "    return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9095bf6a-d70a-4e1d-9007-7e5cb546e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multitask_model(model, test_data, test_labels, test_sentiments,\n",
    "                            class_names=None, sentiment_names=None,\n",
    "                            device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate the multi-task model on both classification and sentiment analysis\n",
    "    \n",
    "    Args:\n",
    "        model: MultiTaskSentenceTransformer model\n",
    "        test_data: List of test sentences\n",
    "        test_labels: List of test labels (numeric indices)\n",
    "        test_sentiments: List of test sentiment labels (numeric indices)\n",
    "        class_names: List of class names for classification\n",
    "        sentiment_names: List of sentiment class names\n",
    "        device: Device to use for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (classification_report, sentiment_report)\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_class_preds = []\n",
    "    all_sent_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence in test_data:\n",
    "            # Tokenize single sentence\n",
    "            inputs = model.sentence_transformer.tokenizer(\n",
    "                sentence, padding=True, truncation=True, \n",
    "                return_tensors=\"pt\", max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            class_logits, sentiment_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "            \n",
    "            # Convert to predictions\n",
    "            class_pred = torch.argmax(class_logits, dim=1).item()\n",
    "            sent_pred = torch.argmax(sentiment_logits, dim=1).item()\n",
    "            \n",
    "            all_class_preds.append(class_pred)\n",
    "            all_sent_preds.append(sent_pred)\n",
    "\n",
    "    # Print predictions\n",
    "    print(\"\\nPredictions:\")\n",
    "    for i, (sentence, true_label, true_sent, pred_label, pred_sent) in enumerate(zip(test_data,\n",
    "                                                            [class_names[i] for i in test_labels],\n",
    "                                                            [sentiment_names[i] for i in test_sentiments],\n",
    "                                                            [class_names[i] for i in all_class_preds],\n",
    "                                                            [sentiment_names[i] for i in all_sent_preds])):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"True class: {true_label}\")\n",
    "        print(f\"True sentiment: {true_sent}\")\n",
    "        print(f\"Predicted class: {pred_label}\")\n",
    "        print(f\"Predicted sentiment: {pred_sent}\")\n",
    "        print()\n",
    "    \n",
    "    # Generate classification reports\n",
    "    class_report = classification_report(\n",
    "        test_labels, all_class_preds, target_names=class_names, zero_division=0\n",
    "    )\n",
    "    \n",
    "    sent_report = classification_report(\n",
    "        test_sentiments, all_sent_preds, target_names=sentiment_names, zero_division=0\n",
    "    )\n",
    "    \n",
    "    return class_report, sent_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f09369cd-1f1f-48c6-97a2-4b1cdc29a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multi-task model...\n",
      "Epoch 1/20\n",
      "Classification loss: 2.4304 | Sentiment loss: 1.6394\n",
      "Epoch 2/20\n",
      "Classification loss: 2.4183 | Sentiment loss: 1.6333\n",
      "Epoch 3/20\n",
      "Classification loss: 2.4053 | Sentiment loss: 1.6251\n",
      "Epoch 4/20\n",
      "Classification loss: 2.3956 | Sentiment loss: 1.6247\n",
      "Epoch 5/20\n",
      "Classification loss: 2.3775 | Sentiment loss: 1.6092\n",
      "Epoch 6/20\n",
      "Classification loss: 2.3644 | Sentiment loss: 1.6013\n",
      "Epoch 7/20\n",
      "Classification loss: 2.3396 | Sentiment loss: 1.5905\n",
      "Epoch 8/20\n",
      "Classification loss: 2.3221 | Sentiment loss: 1.5659\n",
      "Epoch 9/20\n",
      "Classification loss: 2.3090 | Sentiment loss: 1.5444\n",
      "Epoch 10/20\n",
      "Classification loss: 2.2615 | Sentiment loss: 1.5214\n",
      "Epoch 11/20\n",
      "Classification loss: 2.2249 | Sentiment loss: 1.4986\n",
      "Epoch 12/20\n",
      "Classification loss: 2.1992 | Sentiment loss: 1.4615\n",
      "Epoch 13/20\n",
      "Classification loss: 2.1525 | Sentiment loss: 1.4128\n",
      "Epoch 14/20\n",
      "Classification loss: 2.0558 | Sentiment loss: 1.3933\n",
      "Epoch 15/20\n",
      "Classification loss: 2.0040 | Sentiment loss: 1.2991\n",
      "Epoch 16/20\n",
      "Classification loss: 1.9253 | Sentiment loss: 1.2614\n",
      "Epoch 17/20\n",
      "Classification loss: 1.8172 | Sentiment loss: 1.1964\n",
      "Epoch 18/20\n",
      "Classification loss: 1.6982 | Sentiment loss: 1.0825\n",
      "Epoch 19/20\n",
      "Classification loss: 1.6348 | Sentiment loss: 1.0190\n",
      "Epoch 20/20\n",
      "Classification loss: 1.4662 | Sentiment loss: 0.9371\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Predictions:\n",
      "Sentence: The new smartphone suffers from poor battery life.\n",
      "True class: Technology\n",
      "True sentiment: Negative\n",
      "Predicted class: Technology\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Sentence: The basketball team played their final match of the season, and it ended in a draw.\n",
      "True class: Sports\n",
      "True sentiment: Neutral\n",
      "Predicted class: Sports\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Sentence: The president was praised for introducing a new bill to tackle climate change.\n",
      "True class: Politics\n",
      "True sentiment: Positive\n",
      "Predicted class: Politics\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: The movie was disappointing and received poor reviews from critics and audiences alike.\n",
      "True class: Entertainment\n",
      "True sentiment: Negative\n",
      "Predicted class: Entertainment\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Sentence: Regular exercise and a good sleep schedule are essential for maintaining good health.\n",
      "True class: Health\n",
      "True sentiment: Neutral\n",
      "Predicted class: Health\n",
      "Predicted sentiment: Neutral\n",
      "\n",
      "Sentence: The software update is amazing!\n",
      "True class: Technology\n",
      "True sentiment: Positive\n",
      "Predicted class: Entertainment\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: I was sad to see the athlete from Jamaica named Usain Bolt getting disqualified.\n",
      "True class: Sports\n",
      "True sentiment: Negative\n",
      "Predicted class: Entertainment\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: The president will deliver a speech on foreign policy.\n",
      "True class: Politics\n",
      "True sentiment: Neutral\n",
      "Predicted class: Entertainment\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: The tickets for the basketball game sold out within minutes and the game was thrilling to watch.\n",
      "True class: Entertainment\n",
      "True sentiment: Positive\n",
      "Predicted class: Entertainment\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Sentence: The study found a strong correlation between a poor diet and low lifespan.\n",
      "True class: Health\n",
      "True sentiment: Negative\n",
      "Predicted class: Health\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Technology       1.00      0.50      0.67         2\n",
      "       Sports       1.00      0.50      0.67         2\n",
      "     Politics       1.00      0.50      0.67         2\n",
      "Entertainment       0.40      1.00      0.57         2\n",
      "       Health       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           0.70        10\n",
      "    macro avg       0.88      0.70      0.71        10\n",
      " weighted avg       0.88      0.70      0.71        10\n",
      "\n",
      "\n",
      "Sentiment Analysis Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.75      0.75         4\n",
      "     Neutral       1.00      0.33      0.50         3\n",
      "    Positive       0.60      1.00      0.75         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.78      0.69      0.67        10\n",
      "weighted avg       0.78      0.70      0.68        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_test_multitask():\n",
    "    # Classification task setup\n",
    "    class_names = [\"Technology\", \"Sports\", \"Politics\", \"Entertainment\", \"Health\"]\n",
    "    sentiment_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    \n",
    "    # Sample data with both tasks\n",
    "    train_sentences = [\n",
    "        # Technology - Negative\n",
    "        \"The new smartphone suffers from overheating issues and frequent app crashes.\",\n",
    "        # Sports - Neutral\n",
    "        \"The team played their final match of the season, ending in a 1-1 draw.\",\n",
    "        # Politics - Positive\n",
    "        \"The senator was praised for introducing an innovative bill to tackle climate change.\",\n",
    "        # Entertainment - Negative\n",
    "        \"The movie failed to impress critics, receiving mostly poor reviews.\",\n",
    "        # Health - Neutral\n",
    "        \"Regular exercise and a balanced diet are commonly recommended by health professionals.\",\n",
    "        # Technology - Positive\n",
    "        \"The software update significantly boosted performance and added exciting new features.\",\n",
    "        # Sports - Negative\n",
    "        \"The athlete was disqualified after a false start in the 100-meter sprint.\",\n",
    "        # Politics - Neutral\n",
    "        \"The president is scheduled to speak about economic policy at tomorrow’s event.\",\n",
    "        # Entertainment - Positive\n",
    "        \"The concert was a spectacular event, thrilling fans and selling out in minutes.\",\n",
    "        # Health - Negative\n",
    "        \"The study highlighted how poor sleep habits can impair brain function over time.\"\n",
    "    ]\n",
    "    \n",
    "    test_sentences = [\n",
    "        \"The new smartphone suffers from poor battery life.\",\n",
    "        \"The basketball team played their final match of the season, and it ended in a draw.\",\n",
    "        \"The president was praised for introducing a new bill to tackle climate change.\",\n",
    "        \"The movie was disappointing and received poor reviews from critics and audiences alike.\",\n",
    "        \"Regular exercise and a good sleep schedule are essential for maintaining good health.\",\n",
    "        \"The software update is amazing!\",\n",
    "        \"I was sad to see the athlete from Jamaica named Usain Bolt getting disqualified.\",\n",
    "        \"The president will deliver a speech on foreign policy.\",\n",
    "        \"The tickets for the basketball game sold out within minutes and the game was thrilling to watch.\",\n",
    "        \"The study found a strong correlation between a poor diet and low lifespan.\"\n",
    "    ]\n",
    "    \n",
    "    # Train Labels\n",
    "    train_cls_labels = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "    train_sentiment_labels = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]\n",
    "    \n",
    "    # Test Labels\n",
    "    test_cls_labels = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "    test_sentiment_labels = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]\n",
    "    \n",
    "    # Initialize the model\n",
    "    sentence_transformer = SentenceTransformer(\n",
    "        model_name='bert-base-uncased',\n",
    "        pooling_strategy='cls',\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    model = MultiTaskSentenceTransformer(\n",
    "        sentence_transformer=sentence_transformer,\n",
    "        num_classes=len(class_names),\n",
    "        num_sentiment_classes=len(sentiment_names),\n",
    "        hidden_layers=[256, 128],\n",
    "        freeze_transformer=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training multi-task model...\")\n",
    "    train_multitask_model(\n",
    "        model, train_sentences, train_cls_labels, train_sentiment_labels,\n",
    "        epochs=20, batch_size=4\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluation:\")\n",
    "    class_report, sent_report = evaluate_multitask_model(\n",
    "        model, test_sentences, test_cls_labels, test_sentiment_labels,\n",
    "        class_names, sentiment_names\n",
    "    )\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    print(\"\\nSentiment Analysis Report:\")\n",
    "    print(sent_report)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_test_multitask()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
